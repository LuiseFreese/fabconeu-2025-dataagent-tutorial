{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa6c418-bfd0-4af2-ae8b-af01c1d798bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be48666-f60d-4310-932e-12fd34740102",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U fabric-data-agent-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ded55-71b8-4ea6-879c-b82ab072bc78",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# ‚úÖ TASK OVERVIEW\n",
    "\n",
    "**üéØ Goal**  \n",
    "Configure your **Data Agent Artifact** using AI instructions, data source setup, and few-shot examples ‚Äî aiming for **100% correctness** on all questions for the Dataset of your choice.\n",
    "\n",
    "## üìù Pre-Requisite\n",
    "**Ensure you have completed Step 1 in the Lab 3 Curate Your Data Agent Document**\n",
    "The DataAgent created in Step 1 of the \"Lab 3 - Evaluate Your Data Agent\" Document will be the DataAgent used for this Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ STEP 1: Use the Evaluation Notebook\n",
    "\n",
    "1. **Update the `YOUR_DATA_AGENT_ARTIFACT_NAME`**  \n",
    "   - Replace the placeholder with the name of your Data Agent Artifact from in Step 1 of the \"Lab 3 Curate Your Data Agent\" Document. \n",
    "2. **Update the `YOUR_ACCOUNT_NUMBER` Variable**\n",
    "   - Replace the placeholder with your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3f712-c1f2-44dc-8165-4e650763c5f9",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from fabric.dataagent.client import FabricDataAgentManagement\n",
    "from fabric.dataagent.evaluation import evaluate_data_agent, get_evaluation_summary, get_evaluation_details, get_evaluation_summary_per_question\n",
    "#from fabric.dataagent.evaluation.evaluator_api import add_ground_truth_batch\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "\n",
    "YOUR_DATA_AGENT_ARTIFACT_NAME = \"EvalAgent-AGNT###\"\n",
    "\n",
    "YOUR_ACCOUNT_NUMBER = \"###\"\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "data_agent = FabricDataAgentManagement(YOUR_DATA_AGENT_ARTIFACT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419303e-060e-4740-a146-3f94b15869a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "SELECTABLE_ELEMENT_TYPES = [\n",
    "    \"semantic_model.table\",\n",
    "    \"lakehouse_tables.table\",\n",
    "    \"warehouse_tables.table\",\n",
    "    \"kusto.table\",\n",
    "]\n",
    "\n",
    "tables_list = []\n",
    "\n",
    "res = data_agent.get_datasources()[0].get_configuration()\n",
    "res.get('elements')\n",
    "\n",
    "selected_dbs=[]\n",
    "\n",
    "def is_child_selected(elements) -> bool:\n",
    "    if(elements is None or len(elements) == 0):\n",
    "        return False\n",
    "\n",
    "    res = False\n",
    "    for elem in elements:\n",
    "        if elem.get('type') in SELECTABLE_ELEMENT_TYPES and elem.get(\"is_selected\"):\n",
    "            tables_list.append(elem.get(\"display_name\"))\n",
    "            res = True\n",
    "    return res\n",
    "    return is_child_selected(elem.get('children'))\n",
    "\n",
    "for element in res.get('elements'):\n",
    "    if(is_child_selected(element.get('children'))):\n",
    "        selected_dbs.append(element.get('display_name'))\n",
    "\n",
    "selected_dbs\n",
    "\n",
    "selectedListStr = '(' + ', '.join(f'\"{item}\"' for item in selected_dbs) + ')'\n",
    "print(selectedListStr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f349f9-22fb-42ce-8856-7c90506f8610",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## üîÑ STEP 2: Understanding the Groundtruth\n",
    "\n",
    "This step takes the **groundtruths** and displays the **expected answers** and their associated **queries**.\n",
    "\n",
    "üí° This is the reference we will use to evaluate your Data Agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148edd10-8d61-433d-b1ad-fd569c5b67b8",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"Files/ground_truth.csv\" \n",
    "\n",
    "groundtruth_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .csv(csv_path)\n",
    "    .filter(f\"benchmark IN {selectedListStr}\")\n",
    "    .limit(1000)\n",
    ")\n",
    "\n",
    "df_pandas_groundtruth = groundtruth_df.toPandas()\n",
    "type(df_pandas_groundtruth)\n",
    "\n",
    "display(df_pandas_groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57a726-ed07-4bf1-b9b0-d1b3a9f2a5d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## üìä STEP 3: Evaluation\n",
    "\n",
    "This is the **actual evaluation step**, where a new **conversation thread** is started with the Data Agent for **each question**.\n",
    "\n",
    "- We use an **LLM-critic approach** to assess the agent‚Äôs answer against the expected answer.  \n",
    "  > üß† *LLM-critic*: A language model is used to **judge** whether the generated response matches the expected one ‚Äî even when the phrasing differs.\n",
    "  \n",
    "- ‚úÖ **Note**: We **do not compare** the SQL queries. Only the **final answer text** is evaluated.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ Evaluation Output Columns\n",
    "\n",
    "| Column           | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| **T**            | ‚úÖ Number of correct answers (matches expected output)                      |\n",
    "| **F**            | ‚ùå Number of incorrect answers                                               |\n",
    "| **?**            | ü§î Unclear ‚Äî the LLM critic couldn‚Äôt decide                                 |\n",
    "| **%**            | ‚úîÔ∏è Percentage of T out of total (T + F + ?)                                 |\n",
    "| **failed thread**| üîó Links to failed threads (for manual inspection)                          |\n",
    "| **question**     | üìù The question from the ground truth                                       |\n",
    "\n",
    "> üîÅ You can use `num_query_repeat` to ask the same question multiple times to assess variability in responses.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Next Steps\n",
    "\n",
    "- Publish your DataAgent\n",
    "- Inspect the threads that **have mismatches**\n",
    "- Update:\n",
    "  - AI instructions\n",
    "  - Data source instructions\n",
    "  - Few-shot examples  \n",
    "- **Re-run the evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb262c-66e8-4cbf-8904-14a2ced9dc8c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The table used to store evaluation results\n",
    "evaluation_table_name = f'evaluation_output_{YOUR_ACCOUNT_NUMBER}'\n",
    "\n",
    "# run evaluation\n",
    "\n",
    "eval_id = evaluate_data_agent(\n",
    "    df = df_pandas_groundtruth,\n",
    "    data_agent_name = YOUR_DATA_AGENT_ARTIFACT_NAME,\n",
    "    data_agent_stage = \"production\",\n",
    "    table_name = evaluation_table_name)\n",
    "\n",
    "df = get_evaluation_summary_per_question(eval_id, verbose=True, table_name = evaluation_table_name)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
